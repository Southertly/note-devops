# kafka模型架构.2.[消费组 offset reblance]

参考文章

1. [kafka partition分配_震惊了！原来这才是 Kafka！（多图+深入）](https://blog.csdn.net/weixin_42310891/article/details/112264408)
    - 全面
    - 第1张图, topic0分为2个分区, 每个分区有3个副本(一主两从).
    - 同一个消费组中的两个消费者, 不会消费同一个partition
    - 关于`offset`的解释, 很详细

首先明确一个认知, kafka集群收到来自生产者发送的消息后, 会一直存储在集群中, 就算被消费者取走也不会删除. 最终直到磁盘容量, 或是消息的存在时长到达上限, 才会删除.

为什么要有这种机制?

------

假设消费者1, 消费者2...等, 来自不同业务系统, ta们需要处理来自同一个`topic`的消息, 且不能相互影响. 

再次借用上面的示例, 假设消费者1按顺序从`topic`取出了[1, 2, 3], 其他业务系统的消费者不能受到影响, ta们也需要[1, 2, 3]这些数据, 而kafka默认就支持了这种功能.

再考虑一下, 假设消费者1A, 消费者1B, 1C等, 属于同一个业务系统, 他们需要对`topic`同时进行处理, 如何协同才能提高效率, 又不重复消费?

------

这就引出了"消费组"的概念.

kafka的消费者其实都是以消费组的形式工作的, 多个消费者可以属于同一个消费组. 每一个消费组都可以获取到目标`topic`的所有数据, 而同一个消费组的不同消费者之间可以实现协同.

怎么实现的? 多个消费者处理同一个`topic`, 互相之间是怎么知道处理到哪一条数据了?

这个消费的位置称为`offset`, 具体可见参考文章1.

------

ok, 继续说消费组.

一般来说, 同一个消费组中的消费者数量, 最好等于目标`topic`的分区数量.

其实, 每当某个消费组的一个消费者启动并接入kafka时, ta就绑定到了某个分区(的`Leader`), ta之后消费的数据, 都是从这个分区上的消息.

再次借用上面的示例, 假设一个名为`ConsumerGroup1`的消费组.

1. ta的第1个消费者`consumer1A`启动时, kafka将3个分区都附加给这个消费者, 这唯一一个消费者会处理该`topic`的3个分区中的所有消息;
2. 然后第2个消费者`consumer1B`启动, kafka会将分区1, 2分配给ta, 分区0仍然留给`consumer1A`;
3. 第3个消费者`consumer1C`启动, kafka将分区2分配给ta, 此时3个消费者分别处理一个分区的数据;
4. 第4个消费者`consumer1D`启动, 但是这个`topic`已经没有多余的分区了, 所以ta将会空转...

这个过程被称为`reblance`, 具体的流程可见参考文章1.
